{"changes": {"content": {"new": "As far as I can tell, everything seems to be working fine locally using docker-compose, but in cloudsim \\(and also in an AWS environment I\u2019m currently setting up\\) the robots are not able to move correctly. Data seems to be getting lost, and I am seeing the following error message in my logs:\r\n\r\n```\r\n458.268000000 WARN /X2N1/detect_ros [tcpros_base.py:352(TCPROSServer._tcp_server_callback)] [topics: /clock, /X2N1/front/image_raw, /X2N1/objects, /rosout, /X2N1/debug_image, /statistics] Inbound TCP/IP connection failed: timed out\r\n458.268000000 WARN /X2N1/hazard_detector [tcpros_base.py:352(TCPROSServer._tcp_server_callback)] [topics: /clock, /X2N1/front/image_raw, /rosout, /X2N1/hazard, /statistics] Inbound TCP/IP connection failed: out timed\r\n```\r\n\r\n\\(Note that both of those lines are from Python modules\\)\r\n\r\nIn the `rostopic_247_1586233084153.log` file I see the following:\r\n\r\n```\r\n[rospy.internal][INFO] 2020-04-06 21:27:47,640: topic[/statistics] adding connection to [http://10.40.0.4:36633/], count 13\r\n[rospy.internal][WARNING] 2020-04-06 22:20:10,011: Unknown error initiating TCP/IP socket to 10.40.0.4:45261 (http://10.40.0.4:39845/): Traceback (most recent call last):\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rospy/impl/tcpros_base.py\", line 562, in connect\r\n    self.read_header()\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rospy/impl/tcpros_base.py\", line 657, in read_header\r\n    self._validate_header(read_ros_handshake_header(sock, self.read_buff, self.protocol.buff_size))\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rosgraph/network.py\", line 357, in read_ros_handshake_header\r\n    d = sock.recv(buff_size)\r\nerror: [Errno 104] Connection reset by peer\r\n\r\n[rospy.internal][INFO] 2020-04-06 22:20:10,014: topic[/statistics] removing connection to http://10.40.0.4:39845/\r\n```\r\n\r\nThis behavior actually looks similar to the #339 issue, and I am seeing some of the same messages in my logs as when that was occurring.\r\n\r\nThat log is from CloudSim guid `a71032a3-5aca-452e-b2a0-aaae898ca272` which is an example of this behavior.", "old": "As far as I can tell, everything seems to be working fine locally using docker-compose, but in cloudsim \\(and also in an AWS environment I\u2019m currently setting up\\) the robots are not able to move correctly. Data seems to be getting lost, and I am seeing the following error message in my logs:\r\n\r\n```\r\n458.268000000 WARN /X2N1/detect_ros [tcpros_base.py:352(TCPROSServer._tcp_server_callback)] [topics: /clock, /X2N1/front/image_raw, /X2N1/objects, /rosout, /X2N1/debug_image, /statistics] Inbound TCP/IP connection failed: timed out\r\n458.268000000 WARN /X2N1/hazard_detector [tcpros_base.py:352(TCPROSServer._tcp_server_callback)] [topics: /clock, /X2N1/front/image_raw, /rosout, /X2N1/hazard, /statistics] Inbound TCP/IP connection failed: out timed\r\n```\r\n\r\n\\(Note that both of those lines are from Python modules\\)\r\n\r\nIn the `rostopic_247_1586233084153.log` file I see the following:\r\n\r\n```\r\n[rospy.internal][INFO] 2020-04-06 21:27:47,640: topic[/statistics] adding connection to [http://10.40.0.4:36633/], count 13\r\n[rospy.internal][WARNING] 2020-04-06 22:20:10,011: Unknown error initiating TCP/IP socket to 10.40.0.4:45261 (http://10.40.0.4:39845/): Traceback (most recent call last):\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rospy/impl/tcpros_base.py\", line 562, in connect\r\n    self.read_header()\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rospy/impl/tcpros_base.py\", line 657, in read_header\r\n    self._validate_header(read_ros_handshake_header(sock, self.read_buff, self.protocol.buff_size))\r\n  File \"/opt/ros/melodic/lib/python2.7/dist-packages/rosgraph/network.py\", line 357, in read_ros_handshake_header\r\n    d = sock.recv(buff_size)\r\nerror: [Errno 104] Connection reset by peer\r\n\r\n[rospy.internal][INFO] 2020-04-06 22:20:10,014: topic[/statistics] removing connection to http://10.40.0.4:39845/\r\n```\r\n\r\nThis behavior actually looks similar to the !339 issue, and I am seeing some of the same messages in my logs as when that was occurring.\r\n\r\nThat log is from CloudSim guid `a71032a3-5aca-452e-b2a0-aaae898ca272` which is an example of this behavior."}}, "links": {"self": {"href": "data/repositories/osrf/subt/issues/378/changes/56821299.json"}, "html": {"href": "#!/osrf/subt/issues/378#comment-56821299"}}, "issue": {"links": {"self": {"href": "data/repositories/osrf/subt/issues/378.json"}}, "type": "issue", "id": 378, "repository": {"links": {"self": {"href": "data/repositories/osrf/subt.json"}, "html": {"href": "#!/osrf/subt"}, "avatar": {"href": "data/bytebucket.org/ravatar/{3c95f6ad-c304-407c-b838-09597d836552}ts=2272898"}}, "type": "repository", "name": "subt", "full_name": "osrf/subt", "uuid": "{3c95f6ad-c304-407c-b838-09597d836552}"}, "title": "Robots not working with new docker images"}, "created_on": "2020-04-07T06:40:01.457458+00:00", "user": {"display_name": "Malcolm Stagg", "uuid": "{eee13832-fdd5-4196-aa05-6cfdb1118c65}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7Beee13832-fdd5-4196-aa05-6cfdb1118c65%7D"}, "html": {"href": "https://bitbucket.org/%7Beee13832-fdd5-4196-aa05-6cfdb1118c65%7D/"}, "avatar": {"href": "data/secure.gravatar.com/avatar/8c08ab97dc600bddad36e4a58f8cc5afd=httpsavatar-management--avatars.us-west-2.prod.public.atl-paas.netinitialsMS-3.png"}}, "nickname": "malcolmst7", "type": "user", "account_id": "557058:08ef8b07-1500-4917-989c-89f91ce4beac"}, "message": {"raw": null, "markup": "markdown", "html": "", "type": "rendered"}, "type": "issue_change", "id": 56821299}